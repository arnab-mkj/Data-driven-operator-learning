{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "9w_Kmp4CiGIr"
      },
      "source": [
        "## Motivation\n",
        "\n",
        "First,why to learn JAX?\n",
        "In particular, since there are already so many other deep learning frameworks like [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "hqO3eGjIiGIs"
      },
      "source": [
        "### Pros\n",
        "\n",
        "- JAX is a Python library for accelerator-oriented array computation, designed for high-performance numerical computing and large-scale machine learning\n",
        "- JAX provides a familiar NumPy-style API for ease of adoption by researchers and engineers.\n",
        "- JAX includes composable function transformations for\n",
        "  - just-in-time (JIT) compilation  via Open XLA, an open-source machine learning compiler ecosystem,\n",
        "  - automatic differentiation,\n",
        "  - vectorization,\n",
        "  - parallelization.\n",
        "- The same code executes on multiple backends including CPU, GPU & TPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "3bj8bT_diGIs"
      },
      "source": [
        "### Cons\n",
        "\n",
        "In order to efficiently compile programs just-in-time in JAX, the functions need to be written with certain constraints:\n",
        "- The functions are not allowed to have side-effects, meaning that they are not allowed to affect any variable outside of their namespaces.\n",
        "For instance, in-place operations affect a variable even outside of the function.\n",
        "Moreover, stochastic operations such as `torch.rand(...)` change the global state of pseudo random number generators, which is not allowed in functional JAX (we will see later how JAX handles random number generation).\n",
        "- JAX compiles the functions based on anticipated shapes of all arrays/tensors in the function.\n",
        "  This becomes problematic if the shapes or the program flow within the function depends on the values of the tensor.\n",
        "  For instance, in the operation `y = x[x>3]`, the shape of `y` depends on how many values of `x` are greater than 3.\n",
        "- Since JAX is typically used in the machine learning context, it's standard data type is `float32` (single precision) and not as one might expect `float64` (double precision).\n",
        "  \n",
        "Let's discuss more of these constraints in this notebook.\n",
        "Still, in most common cases of training neural networks, it is straightforward to write functions within these constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "Mx7c9qZpiGIs"
      },
      "source": [
        "### What else to know before we start?\n",
        "\n",
        "Throughout this tutorial, we will draw comparisons to PyTorch.\n",
        "\n",
        "Nevertheless, we will use PyTorch's data loading capabilities sometimes in JAX due to their flexibility.\n",
        "Furthermore, we use:\n",
        "- [Flax](https://flax.readthedocs.io/en/latest/) as a neural network library in JAX,\n",
        "- [Optax](https://optax.readthedocs.io/en/latest/index.html) to implement common deep learning optimizers.\n",
        "\n",
        "let's get started with some basic JAX operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "Tl2YiZTQiGIt"
      },
      "source": [
        "### JAX as NumPy on accelerators\n",
        "\n",
        "Every deep learning framework has its own API for dealing with data arrays. For example, PyTorch uses `torch.Tensor` as data arrays on which it defines several operations like matrix multiplication, taking the mean of the elements, etc. In JAX, this basic API strongly resembles the one of [NumPy](https://numpy.org/), and even has the same name in JAX (`jax.numpy`). So, for now, let's think of JAX as NumPy that runs on accelerators. As a first step, let's import JAX and its NumPy API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUrYKlNMiGIt",
        "outputId": "80295d78-fba3-4d46-bbf9-a1beb22152cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using jax 0.5.2\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "print(\"Using jax\", jax.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "Z3KF5dn1iGIu"
      },
      "source": [
        "The NumPy API of JAX is usually imported as `jnp`, to keep a resemblance to NumPy's import as `np`.\n",
        "In the following subsections, we will discuss the main differences between the classical NumPy API and the one of JAX."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "E9-TN0QSiGIv"
      },
      "source": [
        "#### Device Arrays\n",
        "\n",
        "As a first test, let's create some arbitrary arrays like we would do in NumPy. For instance, let's create an array of zeros with shape `[2,5]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLdSkaRqiGIv",
        "outputId": "0e54364e-f78e-4ec0-c0d8-f8ea2f27310e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "a = jnp.zeros((2, 5), dtype=jnp.float32)\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "E-hjemtliGIw"
      },
      "source": [
        "Similarly, we can create an array with values of 0 to 5 by using `arange`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1LDkSeLiGIw",
        "outputId": "5ec74640-dd90-4fd4-ca9e-2f55f1eaed4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3 4 5]\n"
          ]
        }
      ],
      "source": [
        "b = jnp.arange(6)\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e73JpRXViGIx",
        "outputId": "d050ebd0-1bb9-4e82-b5ef-ded9aa3b8e0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'jaxlib.xla_extension.ArrayImpl'>\n"
          ]
        }
      ],
      "source": [
        "print(type(b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "yHF6dZiViGIx"
      },
      "source": [
        "You can also specify the datatype when creating most arrays by setting the optional parameter `dtype`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KawrL6NkiGIx",
        "outputId": "49cec510-10d3-4755-853b-615e982f5f23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([ 5.,  6.,  7.,  8.,  9., 10.], dtype=float16)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# a one-dimensional array c containing the numbers from 5 to 10 with data type jnp.float16. From now on, when writing \"array\" we mean a JAX array and not a NumPy array.\n",
        "\n",
        "c = jnp.arange(5, 11, dtype=jnp.float16)\n",
        "c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UyXjro_iGIy",
        "outputId": "7e44b258-8d95-4aa7-bb36-ee7076f881e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "jaxlib.xla_extension.ArrayImpl"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# What is the class of the variable b.\n",
        "\n",
        "b.__class__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "1gHNZMmAiGIy"
      },
      "source": [
        "Instead of a simple NumPy array, it shows the type `ArrayImpl` which is what JAX uses to represent arrays.\n",
        "In contrast to NumPy, JAX can execute the same code on different backends – CPU, GPU and TPU.\n",
        "An `ArrayImpl` therefore represents an array which is on one of the backends.\n",
        "\n",
        "**Task**: Similar to PyTorch, we can check the device of the array `b` by calling `b.device`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BKDOQ8liGIy",
        "outputId": "28db44ec-6fde-472d-87ab-4d80dc508dce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Similar to PyTorch, we can check the device of the array b by calling b.device.\n",
        "\n",
        "print(b.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "KUCgvHo0iGIy"
      },
      "source": [
        "Depending on your setup, the output will look different:\n",
        "- If you have a GPU and installed JAX properly, it will show the default GPU device `GpuDevice(id=0)`\n",
        "- If you do this exercise on Google Colab, remember to select a GPU in your runtime environment. Then you should also see `GpuDevice(id=0)` (restart of kernel might be necessary)\n",
        "- Otherwise, the object will be on `CpuDevice(id=0)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "ktUdAoRmiGIz"
      },
      "source": [
        "The function `jax.devices()` will show you all available devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aodZJwc7iGIz",
        "outputId": "5c4b2989-c9bd-4553-da20-62f680aabbfa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[CudaDevice(id=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "jax.devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "nEz2PEOciGIz"
      },
      "source": [
        "If you have only a CPU available, then explicitly *printing* the `device` will reveal some more interesting points:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OdOkjTriGIz",
        "outputId": "6436c38b-52a5-4457-daf0-b0ce771949be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "print(b.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "kU15oJNCiGI0"
      },
      "source": [
        "It returns ``TFRT_CPU_0``, which is an optimized CPU backend. According to [website of the developers](https://github.com/ROCm/tensorflow-runtime), \"TFRT is a new TensorFlow runtime. It aims to provide a unified, extensible infrastructure layer with best-in-class performance across a wide variety of domain specific hardware. It provides efficient use of multithreaded host CPUs, supports fully asynchronous programming models, and focuses on low-level efficiency.\"\n",
        "\n",
        "Therefore, you can work with JAX accelerators whether you have a GPU or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "RASPRQeViGI0"
      },
      "source": [
        "In order to get a variable from a JAX device, we can use `jax.device_get`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FhEdlm7iGI0",
        "outputId": "811c92f0-0f68-4fe5-dcaa-c31c0b978cee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3 4 5]\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "b_cpu = jax.device_get(b)\n",
        "print(b_cpu)\n",
        "print(b_cpu.__class__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "ACop8oPciGI5"
      },
      "source": [
        "Unsurprisingly, a simple CPU-based array is nothing else than a NumPy array, which allows for a simple conversion between the two frameworks! To explicitly push a NumPy array to the accelerator, you can use `jax.device_put`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQFe1mUuiGI5",
        "outputId": "07f6a5d5-17a6-4891-dcee-6a5d5afadd0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device put: <class 'jaxlib.xla_extension.ArrayImpl'> on cuda:0\n"
          ]
        }
      ],
      "source": [
        "b_gpu = jax.device_put(b_cpu)\n",
        "print(f'Device put: {b_gpu.__class__} on {b_gpu.device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "QzAvLsWliGI5"
      },
      "source": [
        "Nicely enough, JAX will handle any device clash itself when you try to perform operations on a NumPy array and a DeviceArray by modeling the output as `ArrayImpl` again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmVhcuPliGI5",
        "outputId": "cb01d6f7-27f3-4a1c-8bb5-f5ad642c67a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  2  4  6  8 10]\n",
            "<class 'jaxlib.xla_extension.ArrayImpl'>\n"
          ]
        }
      ],
      "source": [
        "out = b_cpu + b_gpu\n",
        "print(out)\n",
        "print(out.__class__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "m8pW1iHfiGI5"
      },
      "source": [
        "An important technical detail of running operations on DeviceArrays is that when a JAX function is called, the corresponding operation **takes place asynchronously on the accelerator** when possible.\n",
        "\n",
        "For instance, if we call `out = jnp.matmul(b, b)`, JAX first returns a placeholder array for `out` which may not be filled with the values as soon as the function calls finishes.\n",
        "This way, Python will not block the execution of follow-up statements, but instead only does it whenever we strictly need the value of `out`, for instance for printing or putting it on CPU.\n",
        "PyTorch uses a very similar principle to allow asynchronous computation.\n",
        "For more details, see [JAX - Asynchronous Dispatch](https://jax.readthedocs.io/en/latest/async_dispatch.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "dBpAKdbjiGI6"
      },
      "source": [
        "#### Immutable tensors\n",
        "\n",
        "When we would like to change a NumPy array in-place, like replacing the first element of `b` with `1` instead of `0`, we could simply write `b[0]=1`. However, in JAX, this is not possible. A `DeviceArray` object is *immutable*, which means that no in-place operations are possible. The reason for this goes back to our discussion in the introduction: JAX requires programs to be \"pure\" functions, i.e. no effects on variables outside of the function are allowed. Allowing in-place operations of variables would make the program analysis for JAX's just-in-time compilation difficult. Instead, we can use the expression `b.at[0].set(1)` which, analogous to the in-place operation, returns a new array which is identical to `b`, except that its value at the first position is 1.\n",
        "\n",
        " Let's try that out below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKKnp9WCiGI6",
        "outputId": "077fa4bd-648e-4048-e4c8-af5dcdd88034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original array: [0 1 2 3 4 5]\n",
            "Changed array: [1 1 2 3 4 5]\n"
          ]
        }
      ],
      "source": [
        "print('Original array:', b)\n",
        "\n",
        "\n",
        "print('Changed array:', b.at[0].set(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "3h4EUkupiGI6"
      },
      "source": [
        "However, we said that JAX is very efficient. Isn't creating a new array in this case the opposite? While it is indeed less efficient, it can be made much more efficient with JAX's just-in-time compilation. The compiler can recognize unnecessary array duplications, and replace them with in-place operations again. More on the **just-in-time** compilation later!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "uFcEEMFLiGI7"
      },
      "source": [
        "#### Pseudo Random Numbers in JAX\n",
        "\n",
        "In machine learning, we come across several situations where we need to generate pseudo random numbers, e.g.:\n",
        "- randomly shuffling a dataset\n",
        "- sampling a dropout mask for regularization\n",
        "- training a VAE by sampling from the approximate posterior\n",
        "- many more\n",
        "\n",
        "In libraries like NumPy and PyTorch, the random number generator are controlled by a seed, which we set initially to obtain the same samples every time we run the code (this is why the numbers are not truly random, hence \"pseudo\"-random). However, if we call `np.random.normal()` 5 times consecutively, we will get 5 different numbers since every execution changes the state/seed of the pseudo random number generation (PRNG).\n",
        "\n",
        "In JAX, if we would try to generate a random number with this approach, a function creating pseudo-random number would have an effect outside of it. To prevent this, JAX takes a different approach by explicitly passing and iterating the PRNG state. First, let's create a PRNG for the seed 42:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "zAMwcHrBiGJB"
      },
      "outputs": [],
      "source": [
        "rng = jax.random.key(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "XugzeftDiGJB"
      },
      "source": [
        "Now, we can use this PRNG state to generate random numbers. Since with this state, the random number generation becomes deterministic, we sample the same number every time. This is not the case in NumPy if we set the seed once before both operations:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "aQ0-yFN3iGJB"
      },
      "source": [
        "To compare this with numpy, we import it here as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac2HNbBCiGJC",
        "outputId": "e8c80020-b61f-4ba8-cfc5-787aedcae335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX - Random number 1: -0.028304616\n",
            "JAX - Random number 2: -0.028304616\n",
            "NumPy - Random number 1: 0.4967141530112327\n",
            "NumPy - Random number 2: -0.13826430117118466\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# A non-desirable way of generating pseudo-random numbers...\n",
        "jax_random_number_1 = jax.random.normal(rng)\n",
        "jax_random_number_2 = jax.random.normal(rng)\n",
        "print('JAX - Random number 1:', jax_random_number_1)\n",
        "print('JAX - Random number 2:', jax_random_number_2)\n",
        "\n",
        "# Typical random numbers in NumPy\n",
        "np.random.seed(42)\n",
        "np_random_number_1 = np.random.normal()\n",
        "np_random_number_2 = np.random.normal()\n",
        "print('NumPy - Random number 1:', np_random_number_1)\n",
        "print('NumPy - Random number 2:', np_random_number_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "7R5PSwR5iGJC"
      },
      "source": [
        "Usually, we want to have a behavior like NumPy where we get a different random number every time we sample. To achieve this, we can *split* the PRNG state to get usable subkeys every time we need a new pseudo-random number. We can do this with `jax.random.split(...)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCszb7W_iGJC",
        "outputId": "7d237a5e-8aba-4d25-afd5-f67d88f05073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX new - Random number 1: 0.60576403\n",
            "JAX new - Random number 2: 0.4323065\n"
          ]
        }
      ],
      "source": [
        "rng, subkey1, subkey2 = jax.random.split(rng, num=3)  # We create 3 new keys\n",
        "jax_random_number_1 = jax.random.normal(subkey1)\n",
        "jax_random_number_2 = jax.random.normal(subkey2)\n",
        "print('JAX new - Random number 1:', jax_random_number_1)\n",
        "print('JAX new - Random number 2:', jax_random_number_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "_UeMirFGiGJC"
      },
      "source": [
        "Every time you run this cell, you will obtain different random numbers for both operations since we create new PRNG states before sampling and update `rng` itself.\n",
        "\n",
        "**Advice**: In general, you want to split the PRNG key every time before generating a pseudo-number, to prevent accidentally obtaining the exact same numbers (for instance, sampling the exact same dropout mask every time you run the network makes dropout itself quite useless...). For a deeper dive into the ideas behind the random number generation in JAX, see JAX's tutorial on [Pseudo Random Numbers](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "qDsoAdKuiGJD"
      },
      "source": [
        "## Automatic differentiation and Just-in-Time compilation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "7WDVVsDaiGJD"
      },
      "source": [
        "### Function transformations with Jaxpr\n",
        "\n",
        "Rosalia Schneider and Vladimir Mikulik summarize the key points of JAX in the [JAX 101 tutorial](https://jax.readthedocs.io/en/latest/jax-101/01-jax-basics.html) as follows:\n",
        "\n",
        "> The most important difference, and in some sense the root of all the rest, is that JAX is designed to be functional, as in functional programming. The reason behind this is that the kinds of program transformations that JAX enables are much more feasible in functional-style programs. [...] The important feature of functional programming to grok when working with JAX is very simple: don’t write code with side-effects.\n",
        "\n",
        "Essentially, we want to write our main code of JAX in functions that do not affect anything else besides its outputs. For instance, we do not want to change input arrays in-place, or access global variables. While this might seem limiting at first, you get used to this quite quickly and most JAX functions that need to fulfill these constraints can be written this way without problems. Note that not all possible functions in training a neural network need to fulfill the constraints. For instance, loading or saving of models, the logging, or the data generation can be done in naive functions. Only the execution of the machine learning model, which we want to do very efficiently on our accelerator (GPU or TPU), should strictly follow these constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "pqWiMWBZiGJD"
      },
      "source": [
        "What does make JAX functions so special, and how can we think about them? A good way of gaining understanding in how JAX handles function is to understand its intermediate representation: jaxpr. Conceptually, you can think of any operation that JAX does on a function, as first trace-specializing the Python function to be transformed into a small and well-behaved intermediate form. This means that we check which operations are performed on which array, and what shapes the arrays are. Based on this representation, JAX then interprets the function with transformation-specific interpretation rules, which includes automatic differentiation or compiling a function in XLA (meaning Accelerated Linear Algebra) to efficiently use the accelerator.\n",
        "\n",
        "To illustrate this intermediate representation, let's consider a simple function to discuss the concept of dynamic computation graphs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "_kRhZxG5iGJE"
      },
      "source": [
        "Using common NumPy operations in JAX, we can write the function\n",
        "\n",
        "$$\n",
        "y = \\frac{1}{n} \\sum_{i=1}^n\\left[\\left(x_i+2\\right)^2+3\\right]\n",
        "$$\n",
        "\n",
        "as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-sR95YLiGJE",
        "outputId": "a174dea2-fccc-44c3-90b1-8ec36e73b812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input [0. 1. 2.]\n",
            "Output 12.666667\n"
          ]
        }
      ],
      "source": [
        "def simple_graph(x):\n",
        "    x = x + 2\n",
        "    x = x ** 2\n",
        "    x = x + 3\n",
        "    y = x.mean()\n",
        "    return y\n",
        "\n",
        "inp = jnp.arange(3, dtype=jnp.float32)\n",
        "print('Input', inp)\n",
        "print('Output', simple_graph(inp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "cZPe230viGJE"
      },
      "source": [
        "To view the jaxpr representation of this function, we can use `jax.make_jaxpr`. Since the tracing depends on the shape of the input, we need to pass an input to the function (here of shape `[3]`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLdkUfuriGJE",
        "outputId": "efe21d76-4f17-407c-906e-c67efe98f122"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:f32[3]. let\n",
              "    b:f32[3] = add a 2.0\n",
              "    c:f32[3] = integer_pow[y=2] b\n",
              "    d:f32[3] = add c 3.0\n",
              "    e:f32[] = reduce_sum[axes=(0,)] d\n",
              "    f:f32[] = div e 3.0\n",
              "  in (f,) }"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "jax.make_jaxpr(simple_graph)(inp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "UPabTl0hiGJF"
      },
      "source": [
        "A jaxpr representation follows the structure:\n",
        "\n",
        "```python\n",
        "jaxpr ::= { lambda Var* ; Var+.\n",
        "            let Eqn*\n",
        "            in  [Expr+] }\n",
        "```\n",
        "where `Var*` are constants and `Var+` are input arguments. In the cell above, this is `a:f32[3]`, i.e. an array of shape 3 with type `jnp.float32` (`inp`). The list of equations, `Eqn*`, define the intermediate results of the function. You can see that each operation in `simple_graph` is translated to a corresponding equation, like `x = x + 2` is translated to `b:f32[3] = add a 2.0`. Furthermore, you see the specialization of the operations on the input shape, like `x.mean()` being replacing in `e` and `f` with summing and dividing by 3. Finally, `Expr+` in the jaxpr representation are the outputs of the functions. In the example, this is `f`, i.e. the final result of the function.\n",
        "Based on these atomic operations, JAX offers all kind of function transformations, of which we will discuss the most important ones later in this section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFm_Uo2ziGJF",
        "outputId": "70fe8fad-601d-4cca-f336-bfd5c1e5fa59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:f32[3]. let\n",
              "    b:f32[3] = add a 2.0\n",
              "    c:f32[3] = integer_pow[y=2] b\n",
              "    d:f32[3] = add c 3.0\n",
              "    e:f32[] = reduce_sum[axes=(0,)] d\n",
              "    f:f32[] = div e 3.0\n",
              "  in (f,) }"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# You probably know, that one can write the simple function way shorter. How does the jaxpr looks in this case?\n",
        "\n",
        "def simple_graph_short(x):\n",
        "  return ((x + 2)**2 + 3).mean()\n",
        "\n",
        "jax.make_jaxpr(simple_graph_short)(inp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "d1SE-R4diGJG"
      },
      "source": [
        "Chances are high, that your `jaxpr` representation looks identical to the one above. Note that the final division again depends on the input size. In this case, we devide the variable by `3.0` since our input array is of length `3`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "gByrMbudiGJG"
      },
      "source": [
        "Hence, you can consider the `jaxpr` representation as an intermediate compilation stage of JAX. What happens if we actually try to look at the jaxpr representation of a function with **side-effect**?\n",
        "Let's consider the following function, which, as an illustrative example, appends the input to a global list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKrXrHbFiGJH",
        "outputId": "78233894-5900-4efe-df0f-4e7687f55d6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:f32[3]. let\n",
              "    b:f32[3] = integer_pow[y=2] a\n",
              "    c:f32[] = reduce_sum[axes=(0,)] b\n",
              "    d:f32[] = sqrt c\n",
              "  in (d,) }"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "global_list = []\n",
        "\n",
        "# Invalid function with side-effect\n",
        "def norm(x):\n",
        "    global_list.append(x)\n",
        "    x = x ** 2\n",
        "    y = x.sum()\n",
        "    y = jnp.sqrt(y)\n",
        "    return y\n",
        "\n",
        "jax.make_jaxpr(norm)(inp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "XYcyu6YWiGJH"
      },
      "source": [
        "As you can see, the `jaxpr` representation of the function does not contain any operation for `global_list.append(x)`. This is because `jaxpr` only understand side-effect-free code, and cannot represent such effects.\n",
        "\n",
        "Thus, we need to stick with pure functions without any side effects, to prevent any unwanted errors in our functions. If you are interested in learning more about the `jaxpr` representation, check out the [JAX documentation](https://jax.readthedocs.io/en/latest/jaxpr.html) on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBQ3zW3fiGJH",
        "outputId": "a252bf3d-ce7d-443a-c508-d30e2fd29dee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace>,\n",
              " Array([0., 1., 2.], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "#  execute the function norm once on the input inp\n",
        "\n",
        "norm(inp)\n",
        "global_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "7IFe8Qi6iGJI"
      },
      "source": [
        "You see that it is important to stick with pure functions without side effects. But for this tutorial, we just need the basics as discussed above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "WwzeVKROiGJI"
      },
      "source": [
        "### Automatic differentiation\n",
        "\n",
        "The intermediate jaxpr representation defines a computation graph, on which we can perform an essential operation of deep learning framework: automatic differentiation.\n",
        "\n",
        "In frameworks like PyTorch with a dynamic computation graph, we would compute the gradients based on the loss tensor itself, e.g. by calling `loss.backward()`. However, JAX directly works with functions. Instead of backpropagating gradients through tensors, JAX takes as input a function, and outputs another function which directly calculates the gradients for it. While this might seem quite different to what you are used to from other frameworks, it is quite intuitive: your gradient of parameters is really a function of parameters and data.\n",
        "\n",
        "The transformation that allows us to do this is `jax.grad`, which takes as input the function, and returns another function representing the gradient calculation of the (first) input with respect to the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKymrybSiGJJ",
        "outputId": "189fc186-dbe7-4e25-c045-86f8ddec1b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of simple_graph: [1.3333334 2.        2.6666667]\n"
          ]
        }
      ],
      "source": [
        "# Use the function jax.grad to differentiate the function simple_graph. Evaluate its gradient for the input inp.\n",
        "\n",
        "grad_simple_graph = jax.grad(simple_graph)\n",
        "print('Gradient of simple_graph:', grad_simple_graph(inp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "coCetnBjiGJJ"
      },
      "source": [
        "The gradient we get here is exactly the one we would obtain when doing the calculation by hand. Moreover, we can also print the jaxpr representation of the gradient function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8ZVqAlYiGJJ",
        "outputId": "e9eb4beb-af49-42b6-f527-c62857634464"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:f32[3]. let\n",
              "    b:f32[3] = add a 2.0\n",
              "    c:f32[3] = integer_pow[y=2] b\n",
              "    d:f32[3] = integer_pow[y=1] b\n",
              "    e:f32[3] = mul 2.0 d\n",
              "    f:f32[3] = add c 3.0\n",
              "    g:f32[] = reduce_sum[axes=(0,)] f\n",
              "    _:f32[] = div g 3.0\n",
              "    h:f32[] = div 1.0 3.0\n",
              "    i:f32[3] = broadcast_in_dim[\n",
              "      broadcast_dimensions=()\n",
              "      shape=(3,)\n",
              "      sharding=None\n",
              "    ] h\n",
              "    j:f32[3] = mul i e\n",
              "  in (j,) }"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "jax.make_jaxpr(grad_simple_graph)(inp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "DLjyuuaSiGJJ"
      },
      "source": [
        "This shows a unique property of JAX: we can print out the exact computation graph for determining the gradients. Compared to the original function, you can see new equations like `d:f32[3] = integer_pow[y=1] b` and `e:f32[3] = mul 2.0 d`, which model the intermediate gradient of $\\partial b_i^2/\\partial b_i = 2b_i$. Furthermore, the return value `j` is the multiplication of `e` with $1/3$, which maps to the gradient being:\n",
        "\n",
        "$$ \\frac{\\partial y}{\\partial x_i} = \\frac{2}{3}(x_i + 2)$$\n",
        "\n",
        "Hence, we can not only use JAX to estimate the gradients at a certain input value, but actually return the analytical gradient function which is quite a nice feature of JAX!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "fqbluO55iGJK"
      },
      "source": [
        "Often, we do not only want the gradients, but also the actual output of the function, for instance for logging the loss. This can be efficiently done using `jax.value_and_grad`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MriI7_XuiGJK",
        "outputId": "784221c8-dffa-42c1-c7d5-a5622fc2aa54"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array(12.666667, dtype=float32),\n",
              " Array([1.3333334, 2.       , 2.6666667], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "val_grad_function = jax.value_and_grad(simple_graph)\n",
        "val_grad_function(inp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "vfWxCtxviGJK"
      },
      "source": [
        "Of course, you can also look at the `jaxpr` representation of the `val_grad_function`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWnqD2C7iGJK",
        "outputId": "fd2b93a7-3292-4865-a64e-316d5f9a09d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:f32[3]. let\n",
              "    b:f32[3] = add a 2.0\n",
              "    c:f32[3] = integer_pow[y=2] b\n",
              "    d:f32[3] = integer_pow[y=1] b\n",
              "    e:f32[3] = mul 2.0 d\n",
              "    f:f32[3] = add c 3.0\n",
              "    g:f32[] = reduce_sum[axes=(0,)] f\n",
              "    h:f32[] = div g 3.0\n",
              "    i:f32[] = div 1.0 3.0\n",
              "    j:f32[3] = broadcast_in_dim[\n",
              "      broadcast_dimensions=()\n",
              "      shape=(3,)\n",
              "      sharding=None\n",
              "    ] i\n",
              "    k:f32[3] = mul j e\n",
              "  in (h, k) }"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "jax.make_jaxpr(val_grad_function)(inp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "69E_0yQYiGJK"
      },
      "source": [
        "Note that the only difference comes after line\n",
        "\n",
        "    g:f32[] = reduce_sum[axes=(0,)] f\n",
        "\n",
        "where the `grad_function` ''throws away'' the variable which represents the value\n",
        "\n",
        "    _:f32[] = div g 3.0\n",
        "    \n",
        "while the `val_grad_function` assigns it to\n",
        "    \n",
        "    h:f32[] = div g 3.0\n",
        "and finally returns it as its first argument."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "MFuI3xfPiGJL"
      },
      "source": [
        "Further, we can specialize the gradient function to consider multiple input arguments, and add extra outputs that we may not want to differentiate (for instance the accuracy in classification). We will visit the most important ones in the network training later, and refer to other great resources for more details ([JAX Quickstart](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#taking-derivatives-with-grad), [Autodiff cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html), [Advanced autodiff](https://jax.readthedocs.io/en/latest/jax-101/04-advanced-autodiff.html)).\n",
        "\n",
        "To train neural networks, we need to determine the gradient for every parameter in the network with respect to the loss. Listing all parameters as input arguments quickly gets annoying and infeasible.\n",
        "JAX offers an elegant data structure to summarize all parameters: a pytree ([documentation](https://jax.readthedocs.io/en/latest/pytrees.html)). A pytree is a container-like object which structures its elements as a tree. For instance, a linear neural network might have its parameters organized similar to:\n",
        "\n",
        "```python\n",
        "params = {\n",
        "    'linear1': {\n",
        "        'weights': ...,\n",
        "        'bias': ...\n",
        "    },\n",
        "    ...\n",
        "}\n",
        "```\n",
        "JAX offers functions to process pytrees efficiently, such as obtaining all leafs (i.e. all parameters in a network) or applying a function on each element. We will come back to these structures when training a full network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "ilkAvHBuiGJL"
      },
      "source": [
        "### Speeding up computation with Just-In-Time compilation\n",
        "\n",
        "Interestingly, from the previous code cell, you can see in the jaxpr representation of the gradient function that calculating the array `f` and scalar `g` are unnecessary. Intuitively, the gradient of taking the mean is independent of the actual output of the mean, hence we could drop `f` and `g` without any drawback. Finding such cases to improve efficiency and optimizing the code to take full advantage of the available accelerator hardware is one of the big selling points of JAX. It achieves that by *compiling functions just-in-time* with [XLA](https://www.tensorflow.org/xla) (Accelerated Linear Algebra), using their jaxpr representation. Thereby, XLA fuses operations to reduce execution time of short-lived operations and eliminates intermediate storage buffers where not needed. For more details, see the [XLA documentation](https://docs.w3cub.com/tensorflow~guide/performance/xla/index).\n",
        "\n",
        "To compile a function, JAX provides the `jax.jit` transformation. We can either apply the transformation directly on a function (as we will do in the next cell), or use the decorator `@jax.jit` before a function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "u3ngrTXSiGJL"
      },
      "outputs": [],
      "source": [
        "jitted_function = jax.jit(simple_graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "Tkn8sqIAiGJM"
      },
      "source": [
        "The `jitted_function` takes the same input arguments as the original function `simple_graph`. Since the jaxpr representation of a function **depends on the input shape**, the *compilation is started once we put the first input in*.\n",
        "\n",
        "**Important**: Note that this also means that for every different shape we want to run the function, a new XLA compilation is needed. This is why it is recommended to use padding in cases where your input shape strongly varies, e.g. in the case of transformer architectures.\n",
        "For now, let's create an array with 1000 random values, on which we apply the jitted function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "v4kjF7gaiGJM"
      },
      "outputs": [],
      "source": [
        "# Create a new random subkey for generating new random values\n",
        "rng, normal_rng = jax.random.split(rng)\n",
        "large_input = jax.random.normal(normal_rng, (1000,))\n",
        "# Run the jitted function once to start compilation\n",
        "_ = jitted_function(large_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "7iQMqmPziGJM"
      },
      "source": [
        "The output is not any different from what you would get from the non-jitted function. However, how is it about the runtime? Let's time both the original and the jitted function. Due to the asynchronous execution on the accelerator, we add `.block_until_ready()` on the output, which blocks the Python execution until the accelerator finished computing the result and hence get an accurate time estimate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbzFLdtJiGJN",
        "outputId": "afb2b3dc-e220-4bbd-fc1c-03dcc24104f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.83 ms ± 418 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "simple_graph(large_input).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "p2JjbOBNiGJN"
      },
      "source": [
        "Without `.block_until_ready()` we get the wrong estimate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlrTeeAziGJN",
        "outputId": "86aa3d00-2bf6-4abe-fc99-1b37ce130f43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "767 µs ± 8.84 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "simple_graph(large_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "xB6AtJp1iGJN"
      },
      "source": [
        "If you are on a CPU, the difference might not be that much, but on a GPU the values typically differ a lot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4VL5cIKiGJN",
        "outputId": "6ebf0da3-adde-4841-ccf0-ec6b3ea10b92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93 µs ± 34.8 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ],
      "source": [
        "# test the jitted function!\n",
        "\n",
        "%%timeit\n",
        "jitted_function(large_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "3f1bo1ymiGJO"
      },
      "source": [
        "We see that the compiled function is almost 10-15x faster (depends on your system)! This is quite an improvement in performance, and shows the potential of compiling functions with XLA.\n",
        "Furthermore, we can also apply multiple transformations on the same function in JAX, such as applying `jax.jit` on a gradient function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "RAKeLpjQiGJO"
      },
      "outputs": [],
      "source": [
        "jitted_grad_function = jax.jit(grad_simple_graph)\n",
        "_ = jitted_grad_function(large_input)  # Apply once to compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "io6JMSJ0iGJO"
      },
      "source": [
        "Let's time the functions once more:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONkxD8q9iGJO",
        "outputId": "a1b3e046-9639-414e-f0b5-ee9f64f5926e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.38 ms ± 665 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "grad_simple_graph(large_input).block_until_ready()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEYWsRGgiGJO",
        "outputId": "726a6a6d-e030-4c32-fc10-d62b2ed02c6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105 µs ± 2.2 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "jitted_grad_function(large_input).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "2ctd8u6tiGJO"
      },
      "source": [
        "Once more, the jitted function is much faster than the original one. Intuitively, this shows the potential speed up we can gain by using `jax.jit` to compile the whole training step of a network.\n",
        "Generally, we want to jit the largest possible chunk of computation to give the compiler maximum freedom.\n",
        "\n",
        "There are situations in which applying jit to a function is not straight-forward, for instance, if an input argument cannot be traced, or you need to use loops that depend on input arguments.\n",
        "To keep the tutorial simple, and since most neural network training functions do not run into these issues, we do not discuss such special cases here.\n",
        "\n",
        "**Some further references on jit-compilation**:\n",
        "- Jax documentation in [JAX 101 Tutorial](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html)\n",
        "- \"To JIT or not to JIT\" part in [Thinking in JAX](https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html#to-jit-or-not-to-jit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "5L6BOg9HiGJO"
      },
      "source": [
        "## Advanced topics - Vectorization and parallelization\n",
        "\n",
        "After reading this tutorial, you might wonder why we left out some key advertisement points of JAX: automatic vectorization, easy parallelization on multiple accelerators, etc.\n",
        "\n",
        "The reason why we did not include them in our previous discussion is that for building simple networks, and actual most models in our tutorials, you do not really need these methods.\n",
        "\n",
        "However, since they can be handy at some times, for instance, if you have access to a large cluster or are faced with functions that are annoying to vectorize, we review them here in a separate section:\n",
        "\n",
        "This part is inspired by this [JAX tutorial](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "7OYOI7YEiGJP"
      },
      "source": [
        "### Automatic Vectorization with vmap\n",
        "\n",
        "In machine learning, we often vectorize methods to efficiently process multiple inputs or batch elements at the same time. Usually, we have to write the code ourselves to support additional dimensions to vectorize over. However, since JAX can already transform functions to run efficiently on accelerators or calculate gradients, it can also automatically vectorize a function.\n",
        "For instance, let's consider a simple linear layer where we write a function for a single input `x` $\\in \\mathbb{R}^n$, a weight matrix `W` $\\in \\mathbb{R}^{n \\times m}$ and a bias vector `b` $\\in \\mathbb{R}^m$.\n",
        "\n",
        "The function should perform the operation:\n",
        "\n",
        "$$\n",
        "x^T \\cdot W + b\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "qVRKEfYAiGJP"
      },
      "outputs": [],
      "source": [
        "def simple_linear(x, w, b):\n",
        "    # We could already vectorize this function with matmul, but as an example,\n",
        "    # let us use a non-vectorized function with same output\n",
        "    return (x[:,None] * w).sum(axis=0) + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dusQ-rxQiGJP",
        "outputId": "1d3d2cd5-39ec-4b41-8c35-a6c4b70e2d01"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([ 0.576886 , -1.1281335, -6.4787636], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# Example inputs\n",
        "rng, x_rng, w_rng, b_rng = jax.random.split(rng, 4)\n",
        "x = jax.random.normal(x_rng, (4,))\n",
        "W = jax.random.normal(w_rng, (4, 3))\n",
        "b = jax.random.normal(b_rng, (3,))\n",
        "\n",
        "simple_linear(x, W, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "sv8sV61GiGJP"
      },
      "source": [
        "Now, we would like the function to support a batch dimension on `x`, which means that we would like to evaluate the function `simple_linear` for multiple inputs `x` at once. Sure, you can do this explicitly in this case, but if your input is an image, a sequence of words/tokens or a tensor, this function can become quite handy.\n",
        "In general, this additional batch dimension is used as the first dimension, i.e. x is then of dimension `batch_size` times `n`.\n",
        "\n",
        "Our naive implementation above does not support this, since we specialized the axis we sum over. So, let's make JAX do the work for us and vectorize the function by using `jax.vmap`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "SoFZOcHviGJP"
      },
      "outputs": [],
      "source": [
        "vectorized_linear = jax.vmap(simple_linear,\n",
        "                             in_axes=(0, None, None),  # Which axes to vectorize for each input\n",
        "                             out_axes=0  # Which axes to map to in the output\n",
        "                            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "OOmCD6x-iGJP"
      },
      "source": [
        "Specifying `None` for the in-axes of the input arguments `w` and `b` means that we do not want to vectorize any of their input dimensions. With this vmap specification, the function `vectorized_linear` now supports an extra batch dimension in `x`! Let's try it out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkNC4yXciGJP",
        "outputId": "57d6ca1b-f6cb-4586-8229-c6189cce2464"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[-3.02174   ,  1.611254  ,  1.9610097 ],\n",
              "       [-4.0471416 , -2.7000258 , -8.010242  ],\n",
              "       [-7.395664  ,  0.65776247, -2.4034204 ],\n",
              "       [-0.86486953,  1.1105072 ,  1.6554346 ],\n",
              "       [-2.8488154 ,  0.6307627 , -2.2816644 ],\n",
              "       [ 2.1689296 , -0.35141712, -4.0698357 ],\n",
              "       [-0.17229062,  1.7955899 ,  0.20093822],\n",
              "       [ 2.5107303 ,  1.6483753 ,  0.42940354],\n",
              "       [-2.1615202 ,  1.2007736 ,  0.1264503 ],\n",
              "       [ 4.538331  ,  0.09821874, -4.511341  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "rng, x_vec_rng = jax.random.split(rng, 2)\n",
        "x_vec = jax.random.normal(x_vec_rng, (10, 4))\n",
        "\n",
        "vectorized_linear(x_vec, W, b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5AMoNV_iGJQ",
        "outputId": "3ebe038e-3a98-4103-8ce0-76103ad082f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[-3.02174   ,  1.611254  ,  1.9610097 ],\n",
              "       [-4.0471416 , -2.7000258 , -8.010242  ],\n",
              "       [-7.395664  ,  0.65776247, -2.4034204 ],\n",
              "       [-0.86486953,  1.1105072 ,  1.6554346 ],\n",
              "       [-2.8488154 ,  0.6307627 , -2.2816644 ],\n",
              "       [ 2.1689296 , -0.35141712, -4.0698357 ],\n",
              "       [-0.17229062,  1.7955899 ,  0.20093822],\n",
              "       [ 2.5107303 ,  1.6483753 ,  0.42940354],\n",
              "       [-2.1615202 ,  1.2007736 ,  0.1264503 ],\n",
              "       [ 4.538331  ,  0.09821874, -4.511341  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# to do it explicitly\n",
        "\n",
        "def vectorized_linear_explicit(x, w, b):\n",
        "  # x is of shape (batch_size, n)\n",
        "  # w is of shape (n, m)\n",
        "  # b is of shape (m,)\n",
        "  return jnp.matmul(x, w) + b\n",
        "\n",
        "# Test the explicit implementation\n",
        "vectorized_linear_explicit(x_vec, W, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "0US7bTyliGJQ"
      },
      "source": [
        "The new function indeed vectorized our code, calculating $N$ applications of the weights and bias to the input. We can also vectorize the code to run multiple inputs `x` on multiple weights `w` and biases `b` by changing the input argument `in_axes` to `(0, 0, 0)`, or simply `0`. Morever, we can again stack multiple function transformations, such as jitting a vectorized function. Further details on `jax.vmap` can be found in this [tutorial](\n",
        "https://jax.readthedocs.io/en/latest/jax-101/03-vectorization.html\n",
        ") and its [documentation](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html?highlight=vmap)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "R1m6PV05iGJQ"
      },
      "source": [
        "### Parallel evaluation with pmap\n",
        "\n",
        "`jax.vmap` vectorizes a function on a single accelerator. But what if we have multiple GPUs or TPUs available? In PyTorch, we can parallelize a model over multiple GPUs using `nn.DistributedDataParallel`. In JAX, this is yet another function transformation: `jax.pmap`. Similar to `jax.vmap`, we can specify over which axes each input should be parallelized. In a network training, we usually want to parallelize over an extra batch dimension in the data, while the parameters are identical for all devices. For more details on `jax.pmap`, see [Parallel Evaluation in JAX](https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "KPRjF-sJiGJQ"
      },
      "source": [
        "### Dynamic shapes\n",
        "\n",
        "JAX has the great advantage of providing just-in-time compilation of functions to speed up the computation. For this, it uses its intermediate representation `jaxpr`, which is specialized to the shapes of the input arguments. However, this also means that a jitted function is specialized to a certain shape, and running the jitted function with a different input shape requires recompiling the function. For instance, consider the following simple function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "ZP5hLFlNiGJR"
      },
      "outputs": [],
      "source": [
        "def my_function(x):\n",
        "    print('Running the function with shape', x.shape)\n",
        "    return x.mean()\n",
        "\n",
        "jitted_function = jax.jit(my_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "PXYkJYcWiGJR"
      },
      "source": [
        "The print statement is only executed once when the function is compiled, and for all consecutive function calls, this print statement will be ignored since it is not part of the jaxpr representation. Let's run the function now with multiple different input shapes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "VPmXOuVniGJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa82fc40-50d7-427c-f4e6-7b62f0100d51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the function with shape (1,)\n",
            "Running the function with shape (2,)\n",
            "Running the function with shape (3,)\n",
            "Running the function with shape (4,)\n",
            "Running the function with shape (5,)\n",
            "Running the function with shape (6,)\n",
            "Running the function with shape (7,)\n",
            "Running the function with shape (8,)\n",
            "Running the function with shape (9,)\n",
            "Running the function with shape (10,)\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    jitted_function(jnp.zeros(i+1,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "WXsoeSGmiGJR"
      },
      "source": [
        "As we can see, the function is compiled for every different input we give it. This can become inefficient if we actually work with many different shapes. However, running the function again with one of the previous input shapes will not require another compilation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "kxj-HFbciGJR"
      },
      "outputs": [],
      "source": [
        "# Running the functions a second time will not print out anything since\n",
        "# the functions are already jitted for the respective input shapes.\n",
        "for i in range(10):\n",
        "    jitted_function(jnp.zeros(i+1,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "qnrreWcPiGJS"
      },
      "source": [
        "If we have a very limited set of different shapes, we do not see a big performance difference. For instance, in our evaluation, the last batch size is smaller than the previous since we have a limited size of the evaluation dataset. However, for other applications, we might encounter this problem much more often, e.g. for **applications in natural language processing (NLP), time series and graphs**.\n",
        "In these cases, it is recommend to pad the batches to prevent many re-compilations (see Flax's [padding guide](https://flax.readthedocs.io/en/latest/howtos/full_eval.html) for details). We briefly review the two scenarios below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "71SHVyPniGJS"
      },
      "source": [
        "#### NLP and time series\n",
        "\n",
        "In Natural Language Processing, our data consist of sentences which greatly vary in size. Already for batching the elements, we need to apply padding, such that the shape of the batch is determined by the largest sentence in the batch. However, this largest length can vary between batches, especially when we shuffle the dataset before each epoch. In PyTorch, this is not a problem, since the dynamic computation graph allows us to stop the computation whenever we need to. In contrast, JAX would need to recompile the forward pass for every single largest sentence length, which can quickly become very expensive. Padding is needed to reduce the number of compilations, but at the same time introduces unnecessary computation. Hence, we have a tradeoff between number of compilations and extra compute per batch. In the extreme case, PyTorch may even become faster than JAX here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "QFw8QgzIiGJS"
      },
      "source": [
        "#### Graphs\n",
        "\n",
        "Similar to NLP, graphs can vary in their size. Often, graphs differ in their number of nodes, but especially in the number of edges. Furthermore, when we start batching the graphs, the variation of node sizes and edge count considerably increases. Again, padding is needed to reduce the number of compilations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "0iVSiDuliGJS"
      },
      "source": [
        "### Debugging in jitted functions\n",
        "\n",
        "During coding, we likely want to debug our model and sometimes print out intermediate values. In JAX, when jitting functions, this is not as straightforward. As we could see from the previous cells, a print statement is only executed once during compilation, and afterwards removed since it is not part of the jaxpr representation. Furthermore, there can be issues when tracking NaNs in your code (see the [sharp bits tutorial](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#debugging-nans)), and errors like out-of-bounds indexing are silently handled on accelerators by returning -1 instead of an error (see the corresponding section in the [sharp bits tutorial](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#out-of-bounds-indexing)). However, if necessary, one can either run the unjitted version of the forward pass first, and even introduce print statements to the jitted version where needed (see [here](https://github.com/google/jax/issues/196) for a great explanation). Still, it is not as straight-forward as in PyTorch, for example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "NzJbNjBLiGJS"
      },
      "source": [
        "### Miscellaneous divergences from NumPy\n",
        "\n",
        "While `jax.numpy` makes replicates the behavior of `NumPy`’s API, there do exist some differences, e.g. the following.\n",
        "You can find out more [in this tutorial](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html#miscellaneous-divergences-from-numpy).\n",
        "\n",
        "**Task**: Observe the different behaviour when executing `np.arange(10)[11]` and `jnp.arange(10)[11]`, resp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "iD1BX47riGJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa38bfd-9238-4634-8a00-1be5be97f936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy Error: index 11 is out of bounds for axis 0 with size 10\n"
          ]
        }
      ],
      "source": [
        "#  Observe the different behaviour when executing np.arange(10)[11] and jnp.arange(10)[11], resp.\n",
        "\n",
        "try:\n",
        "  np.arange(10)[11]\n",
        "except IndexError as e:\n",
        "  print(\"NumPy Error:\", e)\n",
        "\n",
        "try:\n",
        "  jnp.arange(10)[11]\n",
        "except jax.errors.TracerArrayConversionError as e:\n",
        "  print(\"JAX Error:\", e)\n",
        "except IndexError as e:\n",
        "  print(\"JAX Error (IndexError):\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "_-S7bE2iiGJT"
      },
      "source": [
        "Another example of an unsafe cast with differing results is this one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "rD30NA33iGJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "498275cf-26fc-4322-cbcf-2660c7db9170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[254 255   0   1]\n",
            "[254 255 255 255]\n"
          ]
        }
      ],
      "source": [
        "print(np.arange(254.0, 258.0).astype('uint8'))\n",
        "print(jnp.arange(254.0, 258.0).astype('uint8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "T8NqV9ljiGJT"
      },
      "source": [
        "### Double (64bit) precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "49hoDa0ziGJT"
      },
      "source": [
        "JAX by default enforces *single-precision* numbers to mitigate the Numpy API’s tendency to aggressively promote operands to double. This is the desired behavior for many machine-learning applications, but it may catch you by surprise!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "FA3Ty9cKiGJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90b815af-6258-47b4-c37d-a299df6fb162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-2034371596>:1: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'>  is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
            "  x = jax.random.uniform(jax.random.key(0), (1000,), dtype=jnp.float64)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float32')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "x = jax.random.uniform(jax.random.key(0), (1000,), dtype=jnp.float64)\n",
        "x.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "Zixcqii_iGJT"
      },
      "source": [
        "To use double-precision numbers, you need to set the jax_enable_x64 configuration variable **at startup**.\n",
        "You can to either by setting the environment variable `JAX_ENABLE_X64=True` or by setting the appropriate jax option via"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "atLaZDPKiGJU"
      },
      "outputs": [],
      "source": [
        "jax.config.update(\"jax_enable_x64\", True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "Mrto-WoIiGJU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "781b8b4e-e827-4be3-ca9d-02cf63c3e0c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "x = jax.random.uniform(jax.random.key(0), (1000,), dtype=jnp.float64)\n",
        "x.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "_IlvHg80iGJU"
      },
      "source": [
        "**Important**: XLA doesn’t support 64-bit **convolutions** on all backends! Again, most machine learning models are trained and stored in single precision or other types like `float16` or `bfloat16`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}